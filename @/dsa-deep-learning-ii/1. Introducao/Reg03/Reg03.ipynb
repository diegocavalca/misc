{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Deep Learning II</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Regularização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout \n",
    "\n",
    "Hinton, Srivastava, Krizhevsky, Sutskever e Salakhutdinov (2012) apresentaram o algoritmo de regularização chamado Dropout. Embora o Dropout funcione de maneira diferente de L1 e L2, ele atinge o mesmo objetivo - a prevenção do overfitting. No entanto, o algoritmo aborda a tarefa, removendo realmente neurônios e conexões, pelo menos temporariamente. Ao contrário de L1 e L2, nenhuma penalidade de peso é adicionada. O Dropout não procura diretamente treinar pesos pequenos. O Dropout funciona fazendo com que os neurônios ocultos da rede neural não estejam disponíveis durante parte do treinamento. Descartar uma parte da rede neural faz com que a parte restante seja treinada para alcançar ainda uma boa pontuação, mesmo sem os neurônios perdidos. Isso diminui a coadaptação entre os neurônios, o que resulta em menos overfitting. https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "\n",
    "A maioria das estruturas de redes neurais implementam o Dropout como uma camada separada. As camadas de Dropout funcionam como uma camada de rede neural regular e densamente conectada. A única diferença é que as camadas de Dropout periodicamente descartam alguns de seus neurônios durante o treinamento. Você pode usar as camadas de Dropout nas redes normais regulares feedforward. Na verdade, eles também podem se tornar camadas em redes convolucionais LeNET-5.\n",
    "\n",
    "Os hiperparâmetros usuais para uma camada de Dropout são os seguintes:\n",
    "\n",
    "* Contagem de neurônio\n",
    "* Função de Ativação\n",
    "* Probabilidade de Dropout\n",
    "\n",
    "Os hiperparâmetros da função de ativação e contagem de neurônios funcionam exatamente da mesma maneira que os parâmetros correspondentes no tipo de camada densa mencionado anteriormente. A contagem de neurônios simplesmente especifica o número de neurônios na camada de Dropout. A probabilidade de Dropout indica a probabilidade de um neurônio sair durante a iteração do treino. Assim como acontece com uma camada densa, o programa especifica uma função de ativação para a camada de Dropout.\n",
    "\n",
    "![Regularização Dropout Regularization](images/dropout.png \"Regularização Dropout\")\n",
    "\n",
    "Os neurônios descartados e suas conexões são mostrados como linhas tracejadas neste diagrama acima. A camada de entrada tem dois neurônios de entrada, bem como um neurônio de bias. A segunda camada é uma camada densa com três neurônios, bem como um neurônio de bias. A terceira camada é uma camada de Dropout com seis neurônios regulares, embora o programa tenha descartado 50% deles. Enquanto o programa descarta esses neurônios, ele não os calcula nem os treina. No entanto, a rede neural final usará todos esses neurônios para a saída. \n",
    "\n",
    "Como mencionado anteriormente, o programa apenas descarta temporariamente os neurônios. Durante as iterações de treinamento subsequentes, o programa escolhe diferentes conjuntos de neurônios da camada de Dropout. Embora escolhemos uma probabilidade de 50% para o Dropout, o modelo não necessariamente descartará três neurônios. É como se lançássemos uma moeda para cada um dos neurônios candidatos ao Dropout para escolher se esse neurônio deve ser descartado. Você deve saber que o programa nunca deve descartar o neurônio de bias. Somente os neurônios regulares em uma camada de Dropout são candidatos.\n",
    "\n",
    "A implementação do algoritmo de treinamento influencia o processo de descarte de neurônios. O conjunto de Dropout frequentemente muda uma vez por iteração de treinamento ou lote. O programa também pode fornecer intervalos onde todos os neurônios estão presentes. Algumas estruturas de rede neural fornecem hiperparâmetros adicionais para permitir que você especifique exatamente a taxa desse intervalo.\n",
    "\n",
    "Por que o Dropout é capaz de diminuir o overfitting? A resposta é que o Dropout pode reduzir a chance de uma codependência entre dois neurônios. Dois neurônios que desenvolvem uma dependência de código não poderão operar efetivamente quando um for descartado. Como resultado, a rede neural não pode mais depender da presença de cada neurônio e treina em conformidade. Essa característica diminui sua capacidade de memorizar as informações que lhe são apresentadas, forçando assim a generalização.\n",
    "\n",
    "https://keras.io/layers/core/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Função Auxiliares\n",
    "\n",
    "# Converte todos os valores faltantes na coluna especificada para a mediana\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Converte um dataframe Pandas para as entradas x, y que o TensorFlow precisa\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # Descobre o tipo da coluna de destino. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encoding para int. TensorFlow gosta de 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classificação\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regressão\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00054: early stopping\n",
      "Final score (RMSE): 4.666738510131836\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# TensorFlow com Dropout Para Regressão\n",
    "############################################\n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import figure, show\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "df.drop('name',1,inplace=True)\n",
    "missing_median(df, 'horsepower')\n",
    "x,y = to_xy(df,\"mpg\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=45)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim = x.shape[1], kernel_initializer = 'normal', activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(25, input_dim = x.shape[1], kernel_initializer = 'normal', activation = 'relu'))\n",
    "model.add(Dense(10, input_dim = 64, activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer = 'normal'))\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "monitor = EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience = 5, verbose = 1, mode = 'auto')\n",
    "model.fit(x, y, validation_data = (x_test,y_test), callbacks = [monitor], verbose = 0, epochs = 1000)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "# RMSE \n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
